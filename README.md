# AI Assistant: Индексация и ответы на основе локальной LLM

Этот репозиторий демонстрирует простого ассистента, который строит векторный индекс по текстовым данным и отвечает на вопросы, используя модель **Saiga Mistral 7B** с LoRA‑весами. Проект разбит на небольшие модули и может служить отправной точкой для собственных экспериментов с локальными языковыми моделями.

## Возможности
- Предварительная обработка документов и извлечение структурированных данных
- Построение индекса с помощью `LlamaIndex` и `FAISS`
- Конвертация сообщений в формат промпта, подходящий для Saiga
- Загрузка модели с квантованием и подключение LoRA через `peft`
- Проверка корректности пользовательских запросов

## Установка
1. Установите Python 3.10 или новее.
2. Зависимости можно установить командой:
   ```bash
   pip install llama-index peft transformers langchain-huggingface pandas
   ```
3. Получите токен HuggingFace и API‑ключ OpenAI для работы примера.

## Запуск
Пример входной точки находится в `main.py`. Он запрашивает ваши токены, строит индекс из каталога, указанного в переменной окружения `DATA_PATH` (по умолчанию `/kaggle/input/tanebaum-ostin`), и выполняет тестовый запрос.

```bash
python main.py
```

Для совместимости сохранён скрипт `ai_assistant_llm_indexing.py`, который просто вызывает `main()`.

## Структура репозитория
- `prompt_utils.py` – функции для формирования промптов
- `preprocess.py` – утилиты для очистки и обогащения документов
- `model_setup.py` – загрузка модели и создание индекса
- `query_utils.py` – базовая валидация пользовательских запросов
- `main.py` – пример объединения всех компонентов

## Планы на развитие
- Поддержка индексации сразу нескольких наборов документов
- Улучшение скорости поиска и генерации ответов
- Добавление новых форматов вопросов и источников данных

## Стек технологий
- Python
- Hugging Face Transformers и PEFT
- LlamaIndex + FAISS
- LangChain
- Saiga Mistral 7B

---
