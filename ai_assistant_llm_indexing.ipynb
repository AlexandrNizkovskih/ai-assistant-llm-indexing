{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9508924,
          "sourceType": "datasetVersion",
          "datasetId": 5787852
        }
      ],
      "dockerImageVersionId": 30776,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Введение\n",
        "\n",
        "Этот проект представляет собой реализацию системы для обработки запросов с использованием больших языковых моделей (LLM), таких как Saiga Mistral 7b, с интеграцией популярных библиотек для векторных индексов и эмбеддингов, включая Hugging Face, Llama Index и Langchain.\n",
        "Код позволяет эффективно управлять большими текстовыми данными, структурировать и анализировать их, а также выполнять точные и релевантные ответы на запросы пользователей на основе предобученных моделей и векторных представлений."
      ],
      "metadata": {
        "id": "j2U9xI2UJq1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Почему Saiga Mistral 7b?\n",
        "\n",
        "Saiga Mistral 7b — это одна из наиболее продвинутых и сбалансированных моделей для выполнения языковых задач, таких как генерация текста, ответы на вопросы и обработка больших объемов данных.\n",
        "\n",
        "Вот несколько причин, почему Saiga Mistral 7b была выбрана для этого проекта:\n",
        "\n",
        "Компактный размер и мощность: Модель содержит 7 миллиардов параметров, что делает её мощной, но одновременно достаточно компактной, чтобы эффективно работать на современных аппаратных платформах с ограниченными ресурсами (в том числе в режиме квантования).\n",
        "\n",
        "Поддержка квантования: В проекте используется квантование для работы с моделью в 4-битном режиме. Это позволяет существенно снизить нагрузку на GPU и ускорить вычисления без значительных потерь в качестве ответов. Saiga Mistral 7b поддерживает методы, такие как bnb_4bit и nf4, для этого сценария.\n",
        "\n",
        "Качество генерации текста: Модель демонстрирует отличные результаты в задачах генерации связного и осмысленного текста, что важно для систем обработки естественного языка, требующих высокой степени контекстуального понимания.\n",
        "\n",
        "Гибкость дообучения: Модель совместима с методами дообучения, такими как PEFT и LoRA, что делает её легко адаптируемой к специфическим задачам и данным.\n",
        "\n",
        "Русскоязычная поддержка: Saiga Mistral 7b показывает отличные результаты на русскоязычных данных, что критично для проектов, ориентированных на русскоязычную аудиторию."
      ],
      "metadata": {
        "id": "AJSzHh3GJq1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Установка библиотек"
      ],
      "metadata": {
        "id": "KoFe3qZgJq1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install llama_index pyvis Ipython langchain pypdf langchain_community\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-embeddings-langchain\n",
        "!pip install langchain-huggingface\n",
        "!pip install sentencepiece accelerate\n",
        "!pip install -U bitsandbytes\n",
        "!pip install peft\n",
        "!pip install openai"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-30T15:34:06.004399Z",
          "iopub.execute_input": "2024-09-30T15:34:06.004921Z",
          "iopub.status.idle": "2024-09-30T15:37:13.037037Z",
          "shell.execute_reply.started": "2024-09-30T15:34:06.004887Z",
          "shell.execute_reply": "2024-09-30T15:37:13.035911Z"
        },
        "trusted": true,
        "id": "3O4toP8IJq1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек"
      ],
      "metadata": {
        "id": "qk-1KVQSJq1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:37:13.039198Z",
          "iopub.execute_input": "2024-09-30T15:37:13.039585Z",
          "iopub.status.idle": "2024-09-30T15:37:13.045066Z",
          "shell.execute_reply.started": "2024-09-30T15:37:13.039547Z",
          "shell.execute_reply": "2024-09-30T15:37:13.043966Z"
        },
        "trusted": true,
        "id": "B7eBthooJq1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, Document, GPTVectorStoreIndex\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:37:13.046206Z",
          "iopub.execute_input": "2024-09-30T15:37:13.046518Z",
          "iopub.status.idle": "2024-09-30T15:37:34.105699Z",
          "shell.execute_reply.started": "2024-09-30T15:37:13.046484Z",
          "shell.execute_reply": "2024-09-30T15:37:34.104916Z"
        },
        "trusted": true,
        "id": "6HVturkuJq1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Аутентификация HF и OpenAI"
      ],
      "metadata": {
        "id": "d5n8LueiJq1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import getpass\n",
        "HF_TOKEN = getpass.getpass(\"Вставьте ваш токен: \")\n",
        "\n",
        "# Выполняем аутентификацию\n",
        "login(HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:37:34.107910Z",
          "iopub.execute_input": "2024-09-30T15:37:34.108531Z",
          "iopub.status.idle": "2024-09-30T15:39:16.453137Z",
          "shell.execute_reply.started": "2024-09-30T15:37:34.108495Z",
          "shell.execute_reply": "2024-09-30T15:39:16.452224Z"
        },
        "trusted": true,
        "id": "VVAxaMOwJq1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass # для работы с паролями\n",
        "import os      # для работы с окружением и файловой системой\n",
        "\n",
        "# Запрос ввода ключа от OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Введите OpenAI API Key:\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:39:16.454214Z",
          "iopub.execute_input": "2024-09-30T15:39:16.454499Z",
          "iopub.status.idle": "2024-09-30T15:39:21.663903Z",
          "shell.execute_reply.started": "2024-09-30T15:39:16.454467Z",
          "shell.execute_reply": "2024-09-30T15:39:21.663146Z"
        },
        "trusted": true,
        "id": "Na2GlbG-Jq1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как языковая модель saiga_mistral_7b_lora обучена для ведения диалогов, то для нее определены специальные теги.\n",
        "\n",
        "Сообщения к модели строиться по шаблону:\n",
        "\n",
        "< s >{role}\\n{content}< /s >,\n",
        "\n",
        "где content - это текст сообщения к модели, role - одна из возможных ролей:\n",
        "\n",
        "system - системная роль, определяет преднастройки модели\n",
        "user - вопросы от пользователей"
      ],
      "metadata": {
        "id": "vGh4M8AuJq1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def messages_to_prompt(messages):\n",
        "    prompt = \"\"\n",
        "    for message in messages:\n",
        "        if message.role == 'system':\n",
        "            prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'user':\n",
        "            prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'bot':\n",
        "            prompt += f\"<s>bot\\n\"\n",
        "\n",
        "    # ensure we start with a system prompt, insert blank if needed\n",
        "    if not prompt.startswith(\"<s>system\\n\"):\n",
        "        prompt = \"<s>system\\n</s>\\n\" + prompt\n",
        "\n",
        "    # add final assistant prompt\n",
        "    prompt = prompt + \"<s>bot\\n\"\n",
        "    return prompt\n",
        "\n",
        "def completion_to_prompt(completion):\n",
        "    return f\"<s>system\\n</s>\\n<s>user\\n{completion}</s>\\n<s>bot\\n\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:39:21.665029Z",
          "iopub.execute_input": "2024-09-30T15:39:21.665420Z",
          "iopub.status.idle": "2024-09-30T15:39:21.675214Z",
          "shell.execute_reply.started": "2024-09-30T15:39:21.665371Z",
          "shell.execute_reply": "2024-09-30T15:39:21.674166Z"
        },
        "trusted": true,
        "id": "Ro76rH8eJq1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка модели"
      ],
      "metadata": {
        "id": "IkJy8nkFJq1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализована поддержка дообучения модели через PEFT (Parameter-Efficient Fine-Tuning) с использованием метода LoRA, что позволяет адаптировать модель под специфические задачи с минимальными вычислительными затратами"
      ],
      "metadata": {
        "id": "ipICGSO8Jq1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "# Определяем параметры квантования, иначе модель не выполниться в колабе\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Задаем имя модели\n",
        "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
        "\n",
        "# Создание конфига, соответствующего методу PEFT (в нашем случае LoRA)\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Загружаем базовую модель, ее имя берем из конфига для LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,          # идентификатор модели\n",
        "    quantization_config=quantization_config, # параметры квантования\n",
        "    torch_dtype=torch.float16,               # тип данных\n",
        "    device_map=\"auto\"                        # автоматический выбор типа устройства\n",
        ")\n",
        "\n",
        "# Загружаем LoRA модель\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Переводим модель в режим инференса\n",
        "# Можно не переводить, но явное всегда лучше неявного\n",
        "model.eval()\n",
        "\n",
        "# Загружаем токенизатор\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:39:21.676534Z",
          "iopub.execute_input": "2024-09-30T15:39:21.676923Z",
          "iopub.status.idle": "2024-09-30T15:40:56.014373Z",
          "shell.execute_reply.started": "2024-09-30T15:39:21.676887Z",
          "shell.execute_reply": "2024-09-30T15:40:56.013542Z"
        },
        "trusted": true,
        "id": "b-tYPb9rJq1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод конфигурации для модели"
      ],
      "metadata": {
        "id": "8sCXC9bwJq1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "print(generation_config)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:40:56.015642Z",
          "iopub.execute_input": "2024-09-30T15:40:56.016626Z",
          "iopub.status.idle": "2024-09-30T15:40:56.191926Z",
          "shell.execute_reply.started": "2024-09-30T15:40:56.016586Z",
          "shell.execute_reply": "2024-09-30T15:40:56.190972Z"
        },
        "trusted": true,
        "id": "Kvw2AM4LJq1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    model=model,             # модель\n",
        "    model_name=MODEL_NAME,   # идентификатор модели\n",
        "    tokenizer=tokenizer,     # токенизатор\n",
        "    max_new_tokens=generation_config.max_new_tokens, # параметр необходимо использовать здесь, и не использовать в generate_kwargs, иначе ошибка двойного использования\n",
        "    model_kwargs={\"quantization_config\": quantization_config}, # параметры квантования\n",
        "    generate_kwargs = {   # параметры для инференса\n",
        "      \"bos_token_id\": generation_config.bos_token_id, # токен начала последовательности\n",
        "      \"eos_token_id\": generation_config.eos_token_id, # токен окончания последовательности\n",
        "      \"pad_token_id\": generation_config.pad_token_id, # токен пакетной обработки (указывает, что последовательность ещё не завершена)\n",
        "      \"no_repeat_ngram_size\": generation_config.no_repeat_ngram_size,\n",
        "      \"repetition_penalty\": generation_config.repetition_penalty,\n",
        "      \"temperature\": generation_config.temperature,\n",
        "      \"do_sample\": True,\n",
        "      \"top_k\": 50,\n",
        "      \"top_p\": 0.95\n",
        "    },\n",
        "    messages_to_prompt=messages_to_prompt,     # функция для преобразования сообщений к внутреннему формату\n",
        "    completion_to_prompt=completion_to_prompt, # функции для генерации текста\n",
        "    device_map=\"auto\",                         # автоматически определять устройство\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:40:56.193063Z",
          "iopub.execute_input": "2024-09-30T15:40:56.193354Z",
          "iopub.status.idle": "2024-09-30T15:40:56.626701Z",
          "shell.execute_reply.started": "2024-09-30T15:40:56.193323Z",
          "shell.execute_reply": "2024-09-30T15:40:56.625798Z"
        },
        "trusted": true,
        "id": "v7FNdCQLJq1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Работа с базой данных"
      ],
      "metadata": {
        "id": "Gp1S0fprJq1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизация данных производится с использованием многозадачных эмбеддингов sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2, что обеспечивает поддержку нескольких языков, включая русский"
      ],
      "metadata": {
        "id": "sbrtGB2WJq1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface  import HuggingFaceEmbeddings\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:40:56.631083Z",
          "iopub.execute_input": "2024-09-30T15:40:56.631843Z",
          "iopub.status.idle": "2024-09-30T15:41:02.715195Z",
          "shell.execute_reply.started": "2024-09-30T15:40:56.631803Z",
          "shell.execute_reply": "2024-09-30T15:41:02.714236Z"
        },
        "trusted": true,
        "id": "MUEKzsDqJq1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка ServiceContext (глобальная настройка параметров LLM)\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:41:02.716466Z",
          "iopub.execute_input": "2024-09-30T15:41:02.717237Z",
          "iopub.status.idle": "2024-09-30T15:41:03.722000Z",
          "shell.execute_reply.started": "2024-09-30T15:41:02.717200Z",
          "shell.execute_reply": "2024-09-30T15:41:03.721145Z"
        },
        "trusted": true,
        "id": "pWvXjv19Jq1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rNhJPFBEJq1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработка текстов и структурирования данных, включая возможность извлечения табличной информации из текстовых документов"
      ],
      "metadata": {
        "id": "swyK3lNxJq1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def extract_structured_data(text):\n",
        "\n",
        "    tables = []\n",
        "    table_pattern = re.compile(r'(\\d+(?:\\.\\d+)?(?:\\s+|,|\\t)\\d+(?:\\.\\d+)?(?:\\s+|,|\\t)\\d+(?:\\.\\d+)?(?:\\s+|,|\\t)\\d+)')\n",
        "\n",
        "    for match in table_pattern.finditer(text):\n",
        "        table_data = match.group(0)\n",
        "        # Разделение строки на столбцы по пробелам или запятым\n",
        "        columns = re.split(r'\\s+|,|\\t', table_data)\n",
        "        tables.append(columns)\n",
        "\n",
        "    if tables:\n",
        "        # Преобразуем в DataFrame для структурированного хранения\n",
        "        df = pd.DataFrame(tables)\n",
        "        return df\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def preprocess_document(doc):\n",
        "    if len(doc.text) < 100:  # Фильтрация по длине документа\n",
        "        return None\n",
        "\n",
        "    # Извлекаем структурированные данные из текста\n",
        "    structured_data = extract_structured_data(doc.text)\n",
        "\n",
        "    # Если данные представлены в виде DataFrame, преобразуем их в список\n",
        "    if isinstance(structured_data, pd.DataFrame):\n",
        "        structured_data = structured_data.to_dict(orient=\"list\")  # Преобразуем в сериализуемый формат\n",
        "\n",
        "    # Возвращаем объект Document с текстом и метаданными\n",
        "    return Document(text=doc.text, metadata={\"structured_data\": structured_data})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:41:03.723317Z",
          "iopub.execute_input": "2024-09-30T15:41:03.723729Z",
          "iopub.status.idle": "2024-09-30T15:41:03.732952Z",
          "shell.execute_reply.started": "2024-09-30T15:41:03.723675Z",
          "shell.execute_reply": "2024-09-30T15:41:03.731979Z"
        },
        "trusted": true,
        "id": "SV60c_v9Jq1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для примера работы загружается книга \"Архитектура компьютера\""
      ],
      "metadata": {
        "id": "b6PT_O4UJq1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка и предобработка документов\n",
        "documents = [preprocess_document(doc) for doc in SimpleDirectoryReader('/kaggle/input/tanebaum-ostin').load_data() if doc]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:43:04.773274Z",
          "iopub.execute_input": "2024-09-30T15:43:04.773727Z",
          "iopub.status.idle": "2024-09-30T15:44:53.895850Z",
          "shell.execute_reply.started": "2024-09-30T15:43:04.773686Z",
          "shell.execute_reply": "2024-09-30T15:44:53.895005Z"
        },
        "trusted": true,
        "id": "QjxTKaEfJq1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Фильтрация None значений после предобработки\n",
        "documents = [doc for doc in documents if doc is not None]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:45:26.815693Z",
          "iopub.execute_input": "2024-09-30T15:45:26.816615Z",
          "iopub.status.idle": "2024-09-30T15:45:26.821271Z",
          "shell.execute_reply.started": "2024-09-30T15:45:26.816573Z",
          "shell.execute_reply": "2024-09-30T15:45:26.820334Z"
        },
        "trusted": true,
        "id": "rOwose2vJq1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Система использует GPTVectorStoreIndex для создания векторных представлений документов, что позволяет эффективно искать и извлекать информацию на основе сходства текстов"
      ],
      "metadata": {
        "id": "bfjvP189Jq1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = GPTVectorStoreIndex.from_documents(\n",
        "\tdocuments\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:45:29.639815Z",
          "iopub.execute_input": "2024-09-30T15:45:29.640212Z",
          "iopub.status.idle": "2024-09-30T15:45:43.275195Z",
          "shell.execute_reply.started": "2024-09-30T15:45:29.640176Z",
          "shell.execute_reply": "2024-09-30T15:45:43.274367Z"
        },
        "trusted": true,
        "id": "LTpluZUdJq1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фильтрация запроса по длинне символов"
      ],
      "metadata": {
        "id": "pguev006Jq1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_query(query, min_length=10, max_length=100):\n",
        "    \"\"\"\n",
        "    Функция классифицирует запрос на короткий, длинный или пустой.\n",
        "\n",
        "    Аргументы:\n",
        "    - query: строка запроса.\n",
        "    - min_length: минимальная длина для \"корректного\" запроса.\n",
        "    - max_length: максимальная длина для \"корректного\" запроса.\n",
        "\n",
        "    Возвращает:\n",
        "    - строка с результатом классификации.\n",
        "    \"\"\"\n",
        "    # Удаляем лишние пробелы\n",
        "    query = query.strip()\n",
        "\n",
        "    # Проверка на пустой запрос\n",
        "    if not query:\n",
        "        return \"Запрос пуст.\"\n",
        "\n",
        "    # Проверка на короткий запрос\n",
        "    if len(query) < min_length:\n",
        "        return f\"Запрос слишком короткий. Длина запроса: {len(query)} символов.\"\n",
        "\n",
        "    # Проверка на длинный запрос\n",
        "    if len(query) > max_length:\n",
        "        return f\"Запрос слишком длинный. Длина запроса: {len(query)} символов.\"\n",
        "\n",
        "    return \"Запрос корректный.\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:45:53.034091Z",
          "iopub.execute_input": "2024-09-30T15:45:53.034967Z",
          "iopub.status.idle": "2024-09-30T15:45:53.041584Z",
          "shell.execute_reply.started": "2024-09-30T15:45:53.034925Z",
          "shell.execute_reply": "2024-09-30T15:45:53.040456Z"
        },
        "trusted": true,
        "id": "i04-JJmqJq1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Проверка работы модели"
      ],
      "metadata": {
        "id": "ud9eTgAPJq1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример запроса"
      ],
      "metadata": {
        "id": "H3y90r13Jq1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Серверы работают под управлением каких операционных систем? Поддерживаются ли UNIX и Windows?\"\n",
        "classify_query(query)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:45:56.527099Z",
          "iopub.execute_input": "2024-09-30T15:45:56.527836Z",
          "iopub.status.idle": "2024-09-30T15:45:56.534535Z",
          "shell.execute_reply.started": "2024-09-30T15:45:56.527770Z",
          "shell.execute_reply": "2024-09-30T15:45:56.533611Z"
        },
        "trusted": true,
        "id": "1t11VhsJJq1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример ответа"
      ],
      "metadata": {
        "id": "ZLRUwQT9Jq1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=10\n",
        ")\n",
        "\n",
        "message_template = f\"\"\"<s>system\n",
        "Ты являешься моделью, которая отвечает только на основании предоставленных источников.\n",
        "Отвечай строго на основе информации из текста.\n",
        "Если нужной информации нет в источнике, ответь: 'я не знаю'. Не добавляй ничего, что не указано в тексте. Не придумывай и не добавляй лишние данные.\n",
        "\n",
        "<s>user\n",
        "Вопрос: {query}\n",
        "Источник:\n",
        "</s>\n",
        "\"\"\"\n",
        "#\n",
        "response = query_engine.query(message_template)\n",
        "#\n",
        "print()\n",
        "print('Ответ:')\n",
        "print(response.response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:46:00.321436Z",
          "iopub.execute_input": "2024-09-30T15:46:00.322340Z",
          "iopub.status.idle": "2024-09-30T15:46:50.176871Z",
          "shell.execute_reply.started": "2024-09-30T15:46:00.322298Z",
          "shell.execute_reply": "2024-09-30T15:46:50.175769Z"
        },
        "trusted": true,
        "id": "qef7h_PaJq1o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}